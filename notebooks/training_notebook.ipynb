{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92c4d77",
   "metadata": {},
   "source": [
    "# Neural Language Model - LSTM Training in VS Code\n",
    "\n",
    "## Assignment 2: Language Modeling with Pride and Prejudice\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Overview\n",
    "This notebook trains three LSTM language models:\n",
    "1. **Underfit Model** - Small capacity, underfits the data\n",
    "2. **Overfit Model** - Large capacity, overfits the data\n",
    "3. **Best Fit Model** - Optimal capacity, generalizes well\n",
    "\n",
    "### ‚ö° GPU Support\n",
    "- **With CUDA GPU**: ~15-20 minutes\n",
    "- **With CPU**: ~60-90 minutes (still acceptable for local testing)\n",
    "\n",
    "### üìÅ Project Structure\n",
    "Make sure you're running this from the `notebooks/` directory with the following structure:\n",
    "```\n",
    "Assignment2/\n",
    "‚îú‚îÄ‚îÄ dataset/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Pride_and_Prejudice-Jane_Austen.txt\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ config.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ dataset.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ generate.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils.py\n",
    "‚îî‚îÄ‚îÄ notebooks/\n",
    "    ‚îî‚îÄ‚îÄ training_notebook.ipynb  (this file)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444f6f01",
   "metadata": {},
   "source": [
    "## üîß Step 1: Verify System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n‚úÖ GPU is ready! Training will be FAST (~15-20 minutes)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No GPU detected - using CPU\")\n",
    "    print(\"Training will take ~60-90 minutes (acceptable for local development)\")\n",
    "\n",
    "print(f\"\\nCurrent Directory: {os.getcwd()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc451e67",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Navigate to Project Root\n",
    "\n",
    "This notebook should be in the `notebooks/` directory. We'll navigate to the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a19f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory (parent of notebooks/)\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == 'notebooks':\n",
    "    project_root = notebook_dir.parent\n",
    "    os.chdir(project_root)\n",
    "    print(f\"‚úÖ Changed to project root: {project_root}\")\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "    print(f\"‚ÑπÔ∏è Already in project root: {project_root}\")\n",
    "\n",
    "# Verify project structure\n",
    "print(\"\\nüìÅ Project structure:\")\n",
    "required_dirs = ['src', 'dataset', 'models', 'results', 'vocab']\n",
    "for dir_name in required_dirs:\n",
    "    exists = os.path.exists(dir_name)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} {dir_name}/\")\n",
    "    if not exists and dir_name in ['models', 'results', 'vocab']:\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        print(f\"     ‚Üí Created {dir_name}/ directory\")\n",
    "\n",
    "# Create subdirectories for results\n",
    "os.makedirs('results/plots', exist_ok=True)\n",
    "os.makedirs('results/metrics', exist_ok=True)\n",
    "os.makedirs('results/logs', exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Setup complete! Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70b3b12",
   "metadata": {},
   "source": [
    "## üîç Step 3: Verify Dataset and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "dataset_path = 'dataset/Pride_and_Prejudice-Jane_Austen.txt'\n",
    "if os.path.exists(dataset_path):\n",
    "    file_size = os.path.getsize(dataset_path) / 1024  # KB\n",
    "    print(f\"‚úÖ Dataset found: {dataset_path} ({file_size:.1f} KB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found: {dataset_path}\")\n",
    "    print(\"   Please ensure Pride_and_Prejudice-Jane_Austen.txt is in the dataset/ folder\")\n",
    "\n",
    "# Verify all source modules\n",
    "print(\"\\nüìö Source modules:\")\n",
    "required_modules = ['config.py', 'dataset.py', 'model.py', 'train.py', \n",
    "                   'evaluate.py', 'generate.py', 'utils.py']\n",
    "for module in required_modules:\n",
    "    module_path = f'src/{module}'\n",
    "    exists = os.path.exists(module_path)\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"  {status} src/{module}\")\n",
    "\n",
    "print(\"\\n‚úÖ All checks complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6608ac",
   "metadata": {},
   "source": [
    "## üì¶ Step 4: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    print(\"‚úÖ All required libraries are installed!\")\n",
    "    print(f\"   - PyTorch: {torch.__version__}\")\n",
    "    print(f\"   - NumPy: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing library: {e}\")\n",
    "    print(\"\\nPlease install dependencies:\")\n",
    "    print(\"   pip install torch numpy matplotlib tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec9f13",
   "metadata": {},
   "source": [
    "## üìä Step 5: Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ef2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to Python path\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from dataset import load_and_preprocess_data, build_vocab, create_dataloaders\n",
    "\n",
    "# Load dataset\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "text = load_and_preprocess_data('dataset/Pride_and_Prejudice-Jane_Austen.txt')\n",
    "\n",
    "print(f\"\\nüìñ Text Statistics:\")\n",
    "print(f\"   Total characters: {len(text):,}\")\n",
    "print(f\"   Total words: {len(text.split()):,}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüìù Sample text (first 200 chars):\")\n",
    "print(\"-\" * 70)\n",
    "print(text[:200])\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"\\nüî§ Building vocabulary...\")\n",
    "vocab = build_vocab(text, min_freq=2)\n",
    "\n",
    "print(f\"\\nüìö Vocabulary Statistics:\")\n",
    "print(f\"   Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"   Most common words: {list(vocab.word_to_idx.keys())[:20]}\")\n",
    "\n",
    "# Save vocabulary\n",
    "import pickle\n",
    "os.makedirs('vocab', exist_ok=True)\n",
    "with open('vocab/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(f\"\\n‚úÖ Vocabulary saved to vocab/vocab.pkl\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39ea5fc",
   "metadata": {},
   "source": [
    "## üîß Step 6: Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for training\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    text=text,\n",
    "    vocab=vocab,\n",
    "    seq_length=35,\n",
    "    batch_size=64,\n",
    "    train_ratio=0.8,\n",
    "    val_ratio=0.1\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA LOADERS CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: 64\")\n",
    "print(f\"Sequence length: 35\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e609767",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 7: Update Configuration for Three Models\n",
    "\n",
    "We'll configure three models with different characteristics:\n",
    "- **Underfit**: Small model, high dropout, few epochs\n",
    "- **Overfit**: Large model, no dropout, many epochs\n",
    "- **Best Fit**: Optimal model, balanced settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d048654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config.py to include three model configurations\n",
    "config_content = '''\n",
    "\"\"\"\n",
    "Configuration settings for LSTM Language Model training\n",
    "Three models: Underfit, Overfit, and Best Fit\n",
    "\"\"\"\n",
    "\n",
    "def get_config(model_type='bestfit'):\n",
    "    \"\"\"\n",
    "    Get model configuration based on model type\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'underfit', 'overfit', or 'bestfit'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Configuration parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Base configuration\n",
    "    base_config = {\n",
    "        'data_path': 'dataset/Pride_and_Prejudice-Jane_Austen.txt',\n",
    "        'vocab_path': 'vocab/vocab.pkl',\n",
    "        'model_save_dir': 'models/',\n",
    "        'results_dir': 'results/',\n",
    "        \n",
    "        # Data parameters\n",
    "        'seq_length': 35,\n",
    "        'min_freq': 2,\n",
    "        'batch_size': 64,\n",
    "        'train_ratio': 0.8,\n",
    "        'val_ratio': 0.1,\n",
    "        'num_workers': 0,  # Use 0 for Windows compatibility\n",
    "        \n",
    "        # Generation parameters\n",
    "        'gen_length': 50,\n",
    "        'temperature': 1.0,\n",
    "    }\n",
    "    \n",
    "    # Model-specific configurations\n",
    "    model_configs = {\n",
    "        'underfit': {\n",
    "            'embedding_dim': 64,\n",
    "            'hidden_dim': 128,\n",
    "            'num_layers': 1,\n",
    "            'dropout': 0.5,\n",
    "            'num_epochs': 10,\n",
    "            'learning_rate': 0.01,  # High learning rate\n",
    "            'grad_clip': 5.0,\n",
    "            'patience': 3,\n",
    "            'save_every': 5,\n",
    "        },\n",
    "        'overfit': {\n",
    "            'embedding_dim': 512,\n",
    "            'hidden_dim': 1024,\n",
    "            'num_layers': 3,\n",
    "            'dropout': 0.1,  # Very low dropout\n",
    "            'num_epochs': 30,\n",
    "            'learning_rate': 0.0005,  # Low learning rate\n",
    "            'grad_clip': 5.0,\n",
    "            'patience': 15,  # High patience\n",
    "            'save_every': 5,\n",
    "        },\n",
    "        'bestfit': {\n",
    "            'embedding_dim': 256,\n",
    "            'hidden_dim': 512,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.4,\n",
    "            'num_epochs': 20,\n",
    "            'learning_rate': 0.001,\n",
    "            'grad_clip': 5.0,\n",
    "            'patience': 5,\n",
    "            'save_every': 5,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if model_type not in model_configs:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}. Choose from {list(model_configs.keys())}\")\n",
    "    \n",
    "    # Merge base config with model-specific config\n",
    "    config = {**base_config, **model_configs[model_type]}\n",
    "    config['model_type'] = model_type\n",
    "    \n",
    "    return config\n",
    "'''\n",
    "\n",
    "# Write updated config\n",
    "with open('src/config.py', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"‚úÖ Configuration updated with three model types!\")\n",
    "print(\"\\nModel configurations:\")\n",
    "print(\"  1. Underfit  - Small model, high dropout\")\n",
    "print(\"  2. Overfit   - Large model, low dropout\")\n",
    "print(\"  3. Best Fit  - Optimal balanced model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d1363",
   "metadata": {},
   "source": [
    "## üöÄ Step 8: Train All Three Models\n",
    "\n",
    "This will train:\n",
    "1. **Underfit Model** (~3-5 minutes on GPU)\n",
    "2. **Overfit Model** (~8-12 minutes on GPU)\n",
    "3. **Best Fit Model** (~5-8 minutes on GPU)\n",
    "\n",
    "**Total time: ~15-25 minutes with GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a154d",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Resume Training - Check Existing Models\n",
    "\n",
    "This cell automatically detects which models are already trained and skips them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "from dataset import load_and_preprocess_data, create_dataloaders\n",
    "from model import LSTMLanguageModel\n",
    "from train import LanguageModelTrainer\n",
    "from evaluate import ModelEvaluator, create_comparison_report\n",
    "from utils import plot_training_curves, plot_model_comparison, plot_perplexity_comparison\n",
    "from config import get_config\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nüîß Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚ö° GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUME MODE - CHECKING EXISTING MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Map model types to actual file names\n",
    "model_file_mapping = {\n",
    "    'underfit': 'small',\n",
    "    'overfit': 'medium',\n",
    "    'bestfit': 'large'\n",
    "}\n",
    "\n",
    "# Check which models are already trained\n",
    "existing_models = []\n",
    "missing_models = []\n",
    "\n",
    "for model_type, file_prefix in model_file_mapping.items():\n",
    "    # Check for both naming conventions\n",
    "    checkpoint_paths = [\n",
    "        f'models/{file_prefix}_model_best.pt',\n",
    "        f'models/{model_type}_model_best.pt'\n",
    "    ]\n",
    "    \n",
    "    found = False\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "                epoch = checkpoint.get('epoch', 'N/A')\n",
    "                val_loss = checkpoint.get('val_loss', 0)\n",
    "                print(f\"‚úÖ {model_type:12} - Found at {checkpoint_path}\")\n",
    "                print(f\"   Epoch {epoch}, Val Loss: {val_loss:.4f}\")\n",
    "                existing_models.append((model_type, checkpoint_path))\n",
    "                found = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  {model_type:12} - Corrupted checkpoint at {checkpoint_path}\")\n",
    "    \n",
    "    if not found:\n",
    "        print(f\"‚ùå {model_type:12} - Not found (will train)\")\n",
    "        missing_models.append(model_type)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   ‚úÖ Already trained: {len(existing_models)} model(s)\")\n",
    "print(f\"   ‚ùå Need to train: {len(missing_models)} model(s)\")\n",
    "print(f\"\\nüéØ Training Plan:\")\n",
    "print(f\"   SKIP:  {[m[0] for m in existing_models]}\")\n",
    "print(f\"   TRAIN: {missing_models}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load and preprocess data (once for all models)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "config = get_config('bestfit')\n",
    "dataset, vocab, vocab_size = load_and_preprocess_data(\n",
    "    config['data_path'],\n",
    "    config['vocab_path'],\n",
    "    config['seq_length'],\n",
    "    config['min_freq']\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(\n",
    "    dataset,\n",
    "    config['train_ratio'],\n",
    "    config['val_ratio'],\n",
    "    config['batch_size'],\n",
    "    config['num_workers']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {len(dataset)} sequences\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Load results from already-trained models\n",
    "all_results = {}\n",
    "all_metrics = {}\n",
    "\n",
    "if existing_models:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚è≠Ô∏è  LOADING {len(existing_models)} EXISTING MODEL(S)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for model_type, checkpoint_path in existing_models:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Store results for later use\n",
    "        all_results[model_type] = {\n",
    "            'train_losses': checkpoint.get('train_losses', []),\n",
    "            'val_losses': checkpoint.get('val_losses', []),\n",
    "            'val_perplexities': checkpoint.get('val_perplexities', []),\n",
    "            'best_epoch': checkpoint.get('epoch', 0),\n",
    "            'best_val_loss': checkpoint.get('val_loss', 0),\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ {model_type}: Loaded from {checkpoint_path}\")\n",
    "\n",
    "# Train only missing models\n",
    "if missing_models:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üöÄ TRAINING {len(missing_models)} MODEL(S)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for idx, model_type in enumerate(missing_models, 1):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"TRAINING {model_type.upper()} MODEL ({idx}/{len(missing_models)})\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Get model-specific config\n",
    "        config = get_config(model_type)\n",
    "        \n",
    "        # Use correct file prefix for saving\n",
    "        file_prefix = model_file_mapping[model_type]\n",
    "        config['model_type'] = file_prefix\n",
    "        \n",
    "        # Create model\n",
    "        model = LSTMLanguageModel(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            hidden_dim=config['hidden_dim'],\n",
    "            num_layers=config['num_layers'],\n",
    "            dropout=config['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        print(f\"\\nüìä {model_type.upper()} Model Architecture:\")\n",
    "        print(f\"  Embedding dim: {config['embedding_dim']}\")\n",
    "        print(f\"  Hidden dim: {config['hidden_dim']}\")\n",
    "        print(f\"  Num layers: {config['num_layers']}\")\n",
    "        print(f\"  Dropout: {config['dropout']}\")\n",
    "        print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "        print(f\"  Epochs: {config['num_epochs']}\")\n",
    "        print(f\"  Total parameters: {model.count_parameters():,}\")\n",
    "        print(f\"  Will save as: models/{file_prefix}_model_best.pt\")\n",
    "        \n",
    "        # Train model\n",
    "        start_time = datetime.now()\n",
    "        trainer = LanguageModelTrainer(\n",
    "            model, train_loader, val_loader, config, device\n",
    "        )\n",
    "        results = trainer.train()\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60\n",
    "        \n",
    "        all_results[model_type] = results\n",
    "        \n",
    "        # Plot training curves\n",
    "        plot_training_curves(\n",
    "            results['train_losses'],\n",
    "            results['val_losses'],\n",
    "            f\"{model_type.capitalize()} Model\",\n",
    "            save_path=f\"results/plots/{model_type}_training_curves.png\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ {model_type.upper()} model training complete!\")\n",
    "        print(f\"   Best Val Loss: {results['best_val_loss']:.4f}\")\n",
    "        print(f\"   Best Epoch: {results['best_epoch']}\")\n",
    "        print(f\"   Training time: {duration:.1f} minutes\")\n",
    "        print(f\"   Saved as: models/{file_prefix}_model_best.pt\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ ALL MODELS ALREADY TRAINED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"To retrain, delete the checkpoint files in models/ folder\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING PHASE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"   Ready: {len(all_results)} model(s)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store model types for next cells\n",
    "model_types = ['underfit', 'overfit', 'bestfit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294bda0",
   "metadata": {},
   "source": [
    "## üìä Step 9: Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06657f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING MODEL COMPARISON PLOTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot validation loss comparison\n",
    "plot_model_comparison(\n",
    "    all_results,\n",
    "    metric='val_losses',\n",
    "    save_path='results/plots/model_comparison.png'\n",
    ")\n",
    "\n",
    "# Plot perplexity comparison\n",
    "plot_perplexity_comparison(\n",
    "    all_results,\n",
    "    save_path='results/plots/perplexity_comparison.png'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Comparison plots created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc41e9",
   "metadata": {},
   "source": [
    "## üéØ Step 10: Evaluate All Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b319763",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ALL MODELS ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_type.upper()} MODEL EVALUATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    config = get_config(model_type)\n",
    "    \n",
    "    # Find model checkpoint (try both naming conventions)\n",
    "    file_prefix = model_file_mapping[model_type]\n",
    "    checkpoint_paths = [\n",
    "        f'models/{file_prefix}_model_best.pt',\n",
    "        f'models/{model_type}_model_best.pt',\n",
    "        f'models/{model_type}_best.pt'\n",
    "    ]\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    for path in checkpoint_paths:\n",
    "        if os.path.exists(path):\n",
    "            checkpoint_path = path\n",
    "            break\n",
    "    \n",
    "    if not checkpoint_path:\n",
    "        print(f\"‚ö†Ô∏è  Model not found! Skipping evaluation.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"üìÇ Loading from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Create model\n",
    "    model = LSTMLanguageModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = ModelEvaluator(model, device)\n",
    "    test_metrics = evaluator.evaluate_on_dataset(test_loader, \"Test\")\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses = all_results[model_type].get('train_losses', [])\n",
    "    final_train_loss = train_losses[-1] if train_losses else checkpoint.get('val_loss', 0)\n",
    "    \n",
    "    all_metrics[model_type] = {\n",
    "        'train': {\n",
    "            'final_loss': final_train_loss\n",
    "        },\n",
    "        'val': {\n",
    "            'loss': checkpoint['val_loss'],\n",
    "            'perplexity': checkpoint['val_perplexity'],\n",
    "        },\n",
    "        'test': test_metrics,\n",
    "        'best_epoch': all_results[model_type]['best_epoch'],\n",
    "        'total_epochs': len(train_losses) if train_losses else checkpoint.get('epoch', 0),\n",
    "        'config': {\n",
    "            'embedding_dim': config['embedding_dim'],\n",
    "            'hidden_dim': config['hidden_dim'],\n",
    "            'num_layers': config['num_layers'],\n",
    "            'dropout': config['dropout'],\n",
    "            'learning_rate': config['learning_rate'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save individual metrics\n",
    "    metrics_path = f\"results/metrics/{model_type}_metrics.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(all_metrics[model_type], f, indent=4)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Metrics saved to: {metrics_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b837a3",
   "metadata": {},
   "source": [
    "## üìà Step 11: Create Final Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison report\n",
    "create_comparison_report(\n",
    "    all_metrics,\n",
    "    'results/metrics/final_comparison.json'\n",
    ")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<12} {'Params':<12} {'Test Loss':<12} {'Test PPL':<12} {'Val Loss':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model_type in model_types:\n",
    "    metrics = all_metrics[model_type]\n",
    "    params = model.count_parameters() if model_type == 'bestfit' else 0\n",
    "    \n",
    "    # Calculate approximate parameters based on config\n",
    "    cfg = metrics['config']\n",
    "    approx_params = (\n",
    "        vocab_size * cfg['embedding_dim'] +  # Embedding\n",
    "        4 * cfg['num_layers'] * cfg['hidden_dim'] * (cfg['embedding_dim'] + cfg['hidden_dim']) +  # LSTM\n",
    "        cfg['hidden_dim'] * vocab_size  # Output layer\n",
    "    )\n",
    "    \n",
    "    print(f\"{model_type:<12} {approx_params:>10,}  \"\n",
    "          f\"{metrics['test']['loss']:>10.4f}  \"\n",
    "          f\"{metrics['test']['perplexity']:>10.2f}  \"\n",
    "          f\"{metrics['val']['loss']:>10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838f1cb",
   "metadata": {},
   "source": [
    "## üìù Step 12: Generate Text Samples\n",
    "\n",
    "Generate text using all three models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate_text\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEXT GENERATION - ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prompts to test\n",
    "start_texts = [\n",
    "    \"it is a truth\",\n",
    "    \"elizabeth was\",\n",
    "    \"mr darcy\"\n",
    "]\n",
    "\n",
    "all_generations = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_type.upper()} MODEL GENERATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    config = get_config(model_type)\n",
    "    \n",
    "    # Find model checkpoint (try both naming conventions)\n",
    "    file_prefix = model_file_mapping[model_type]\n",
    "    checkpoint_paths = [\n",
    "        f'models/{file_prefix}_model_best.pt',\n",
    "        f'models/{model_type}_model_best.pt',\n",
    "        f'models/{model_type}_best.pt'\n",
    "    ]\n",
    "    \n",
    "    checkpoint_path = None\n",
    "    for path in checkpoint_paths:\n",
    "        if os.path.exists(path):\n",
    "            checkpoint_path = path\n",
    "            break\n",
    "    \n",
    "    if not checkpoint_path:\n",
    "        print(f\"‚ö†Ô∏è  Model not found! Skipping text generation.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"üìÇ Loading from: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = LSTMLanguageModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    all_generations[model_type] = {}\n",
    "    \n",
    "    # Generate for each prompt\n",
    "    for start_text in start_texts:\n",
    "        generated = generate_text(\n",
    "            model, vocab, start_text,\n",
    "            max_length=40,\n",
    "            temperature=0.8,\n",
    "            device=device\n",
    "        )\n",
    "        all_generations[model_type][start_text] = generated\n",
    "        \n",
    "        print(f\"\\nüìù '{start_text}':\")\n",
    "        print(f\"   {generated}\")\n",
    "\n",
    "# Save generations\n",
    "with open('results/generated_samples.json', 'w') as f:\n",
    "    json.dump(all_generations, f, indent=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Text generation complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed048e",
   "metadata": {},
   "source": [
    "## üìä Step 13: Display Results\n",
    "\n",
    "View all generated plots and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229171f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all plots\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING RESULTS VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "plot_files = [\n",
    "    ('results/plots/underfit_training_curves.png', 'Underfit Model Training Curves'),\n",
    "    ('results/plots/overfit_training_curves.png', 'Overfit Model Training Curves'),\n",
    "    ('results/plots/bestfit_training_curves.png', 'Best Fit Model Training Curves'),\n",
    "    ('results/plots/model_comparison.png', 'Model Comparison - Validation Loss'),\n",
    "    ('results/plots/perplexity_comparison.png', 'Model Comparison - Perplexity'),\n",
    "]\n",
    "\n",
    "for plot_file, title in plot_files:\n",
    "    if os.path.exists(plot_file):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{title}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        img = Image.open(plot_file)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  {plot_file} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c64c95",
   "metadata": {},
   "source": [
    "## üìÅ Step 14: Verify All Output Files\n",
    "\n",
    "Check that all expected files were created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FILE VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check all expected files\n",
    "expected_files = {\n",
    "    'Models': [\n",
    "        'models/underfit_model_best.pt',\n",
    "        'models/overfit_model_best.pt',\n",
    "        'models/bestfit_model_best.pt'\n",
    "    ],\n",
    "    'Plots': [\n",
    "        'results/plots/underfit_training_curves.png',\n",
    "        'results/plots/overfit_training_curves.png',\n",
    "        'results/plots/bestfit_training_curves.png',\n",
    "        'results/plots/model_comparison.png',\n",
    "        'results/plots/perplexity_comparison.png'\n",
    "    ],\n",
    "    'Metrics': [\n",
    "        'results/metrics/underfit_metrics.json',\n",
    "        'results/metrics/overfit_metrics.json',\n",
    "        'results/metrics/bestfit_metrics.json',\n",
    "        'results/metrics/final_comparison.json'\n",
    "    ],\n",
    "    'Data': [\n",
    "        'vocab/vocab.pkl',\n",
    "        'results/generated_samples.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "for category, files in expected_files.items():\n",
    "    print(f\"\\nüìÇ {category}:\")\n",
    "    for file_path in files:\n",
    "        exists = os.path.exists(file_path)\n",
    "        if exists:\n",
    "            size = os.path.getsize(file_path) / 1024  # KB\n",
    "            print(f\"  ‚úÖ {file_path} ({size:.1f} KB)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {file_path} - NOT FOUND\")\n",
    "            all_good = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if all_good:\n",
    "    print(\"‚úÖ All files created successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some files are missing. Check the training logs above.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc843c9",
   "metadata": {},
   "source": [
    "## üìã Step 15: Training Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45996914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY - ASSIGNMENT 2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Models Trained: 3\")\n",
    "print(\"  1. Underfit Model  - Intentionally limited capacity\")\n",
    "print(\"  2. Overfit Model   - Excessive capacity, prone to overfitting\")\n",
    "print(\"  3. Best Fit Model  - Optimal balance\")\n",
    "\n",
    "print(\"\\nüìà Results Summary:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model_type in model_types:\n",
    "    metrics = all_metrics[model_type]\n",
    "    print(f\"\\n{model_type.upper()} Model:\")\n",
    "    print(f\"  Train Loss: {metrics['train']['final_loss']:.4f}\")\n",
    "    print(f\"  Val Loss:   {metrics['val']['loss']:.4f}\")\n",
    "    print(f\"  Test Loss:  {metrics['test']['loss']:.4f}\")\n",
    "    print(f\"  Test PPL:   {metrics['test']['perplexity']:.2f}\")\n",
    "    print(f\"  Best Epoch: {metrics['best_epoch']}/{metrics['total_epochs']}\")\n",
    "    \n",
    "    # Determine if overfitting/underfitting\n",
    "    train_val_gap = metrics['val']['loss'] - metrics['train']['final_loss']\n",
    "    if train_val_gap > 0.5:\n",
    "        print(f\"  ‚ö†Ô∏è  Overfitting detected (Train-Val gap: {train_val_gap:.4f})\")\n",
    "    elif metrics['val']['loss'] > 5.0:\n",
    "        print(f\"  ‚ö†Ô∏è  Underfitting detected (High validation loss)\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Good fit (Train-Val gap: {train_val_gap:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìÅ All files saved in project directory\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Training complete!\")\n",
    "print(\"‚úÖ Results available in respective folders!\")\n",
    "print(\"‚úÖ Ready for report generation!\")\n",
    "\n",
    "# Show total training time estimate\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n‚ö° GPU training completed successfully!\")\n",
    "else:\n",
    "    print(\"\\nüíª CPU training completed successfully!\")\n",
    "\n",
    "print(\"\\nüìÇ Output Locations:\")\n",
    "print(f\"  - Models: {os.path.abspath('models/')}\")\n",
    "print(f\"  - Plots: {os.path.abspath('results/plots/')}\")\n",
    "print(f\"  - Metrics: {os.path.abspath('results/metrics/')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eaf03f",
   "metadata": {},
   "source": [
    "## üéØ Next Steps for Your Assignment\n",
    "\n",
    "### ‚úÖ What You Now Have:\n",
    "\n",
    "**Trained Models:**\n",
    "- `models/underfit_model_best.pt` - Underfit model checkpoint\n",
    "- `models/overfit_model_best.pt` - Overfit model checkpoint  \n",
    "- `models/bestfit_model_best.pt` - Best fit model checkpoint\n",
    "\n",
    "**Visualizations:**\n",
    "- `results/plots/underfit_training_curves.png` - Training/validation curves\n",
    "- `results/plots/overfit_training_curves.png` - Training/validation curves\n",
    "- `results/plots/bestfit_training_curves.png` - Training/validation curves\n",
    "- `results/plots/model_comparison.png` - Side-by-side comparison\n",
    "- `results/plots/perplexity_comparison.png` - Perplexity comparison\n",
    "\n",
    "**Metrics:**\n",
    "- `results/metrics/underfit_metrics.json` - Detailed metrics\n",
    "- `results/metrics/overfit_metrics.json` - Detailed metrics\n",
    "- `results/metrics/bestfit_metrics.json` - Detailed metrics\n",
    "- `results/metrics/final_comparison.json` - Comparison summary\n",
    "\n",
    "**Generated Text:**\n",
    "- `results/generated_samples.json` - Text samples from all models\n",
    "\n",
    "---\n",
    "\n",
    "### \udcdd For Your Report:\n",
    "\n",
    "**1. Analyze Underfitting (Underfit Model):**\n",
    "   - High training AND validation loss\n",
    "   - Model too simple to capture patterns\n",
    "   - Poor text generation quality\n",
    "\n",
    "**2. Analyze Overfitting (Overfit Model):**\n",
    "   - Low training loss, high validation loss\n",
    "   - Large train-val gap (> 0.5)\n",
    "   - Memorizes training data but poor generalization\n",
    "\n",
    "**3. Analyze Good Fit (Best Fit Model):**\n",
    "   - Balanced train/val loss\n",
    "   - Small train-val gap\n",
    "   - Best test performance\n",
    "   - Good quality generated text\n",
    "\n",
    "**4. Include in Report:**\n",
    "   - All 5 plots from `results/plots/`\n",
    "   - Metrics comparison from `results/metrics/final_comparison.json`\n",
    "   - Generated text samples showing quality differences\n",
    "   - Analysis of why each model behaves differently\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Optional: Further Experiments\n",
    "\n",
    "Run additional cells below to experiment with text generation or analyze specific aspects of your models.\n",
    "\n",
    "**All training is complete!** You can now write your report using the generated files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
